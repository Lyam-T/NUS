{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "febd9490-e847-4a28-8738-9594d538b04e",
   "metadata": {},
   "source": [
    "# CS3244, Machine Learning, Semester 1, 2024/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da488f-a0f3-442e-a598-9a1ec87bd3f5",
   "metadata": {},
   "source": [
    "### Decision Tree to classify Iris flowers\n",
    "\n",
    "To run this exercise, you will need to install the ***pydotplus*** package. Use Anaconda Navigator or the command line to install the package.\n",
    "\n",
    "![Iris image](https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/84f03ae1d048482471f2a9ca85b0c649730cc269/images/03_iris.png \"Image from https://github.com/justmarkham/scikit-learn-videos/blob/master/03_getting_started_with_iris.ipynb\")\n",
    "*Image from https://github.com/justmarkham/scikit-learn-videos/blob/master/03_getting_started_with_iris.ipynb*\n",
    "\n",
    "The Iris dataset contains 50 samples each of three types of the Iris flower: Iris Setosa, Iris Versicolor, and Iris Virginica. For each sample, the following measurements are given: sepal length, sepal width, petal width, petal length.\n",
    "\n",
    "Run the experiment to build a decision tree for classifying Iris flowers based on their measurements.\n",
    "\n",
    "From the visualization of the tree, write down a rule that correctly classifies all cases of Iris Setosa. How many disjunction of rules would cover all cases of positive instances of Iris Versicolor? What accuracy would the decision tree be if pruned to depth 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bbf73f-eea8-4634-98eb-3cfcfd1622fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "from IPython.display import Image\n",
    "import pydotplus \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6da92-1eb9-43db-98c0-1618be8dee7f",
   "metadata": {},
   "source": [
    "### Decision Tree on Australian Credit Dataset\n",
    "\n",
    "The **Australian** credit approval dataset illustrates multiple practical issues.\n",
    "* Missing feature values: This is very common in practice. This can be handled in various ways.\n",
    "     * Removing instances with missing feature values. This may be fine if the number of such instances is small. However, the issue still has to be handled when the predictor is deployed.\n",
    "     * Imputing (predicting and filling in) the missing values. May affect performance if prediction is poor.\n",
    "     * Encoding the missing value as a special value. This may be appropriate if the value is not missing at random and being missing actually provides some information.\n",
    "     * Algorithm specific method. Decision trees have specific methods for handling missing values (e.g. averaging over both paths down the tree at the missing node) but this is not implemented in Scikit Learn. \n",
    "* The Australian dataset has a mix of continuous and categorical feature.\n",
    "* For confidentiality purposes, feature names and values have been changed into meaningless symbols in the dataset. As a result, we cannot use our exploit knowledge of the problem to construct better predictors. One question is how to handle the categorical feature.\n",
    " * If the feature is an ordinal variable, i.e. the values are ordered, it may sometimes be useful to map the values to integers or reals, particularly if we expect the target value to change monotonically with the value of the feature.\n",
    " * If the feature is a nominal variable, i.e. the values cannot be ordered, mapping the values to integers or reals may not make sense. Some decision tree algorithms can do a multiway split at the variable with one child for each possible variable value. However, Scikit Learn simply treats all variables as integers/reals and do binary tests with $\\leq$.\n",
    " \n",
    " \n",
    "In this dataset, missing values are handled by imputation: mean values are used for continuous variables, and mode is used for categorical variables. For datasets like this, one strength of decision trees is that it is somewhat less sensitive to appropriate processing of variables.\n",
    "\n",
    "Before running the experiments, answer the following in Archipelago.\n",
    "\n",
    "Do you think the decision tree algorithm is sensitive to scaling of the variables? Yes or No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fded7c-121f-4b33-b7f7-623f517bd0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = load_svmlight_file(\"australian.txt\")\n",
    "# Should really repeat this with many random splits to get more reliable results; try various splits\n",
    "# by changing random_state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "# Scale and split data\n",
    "scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Decision tree on original data\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"DT accuracy with original data: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "# Decision tree on scaled data\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "clf = clf.fit(X_train_scaled, y_train)\n",
    "predict = clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"DT accuracy with scaled data: \" + \"{0:.2f}\".format(accuracy) + \"\\n\")\n",
    "\n",
    "# Nearest neighbour on original data\n",
    "clf = neighbors.KNeighborsClassifier(1)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"NN accuracy with original data: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "# Nearest neighbour on scaled data\n",
    "clf = neighbors.KNeighborsClassifier(1)\n",
    "clf = clf.fit(X_train_scaled, y_train)\n",
    "predict = clf.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"NN accuracy with scaled data: \" + \"{0:.2f}\".format(accuracy) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e332c3b2-2604-483f-865d-2832efcc84a9",
   "metadata": {},
   "source": [
    "### Pruning, Bagging and Boosting Decision Trees on Australian Credit\n",
    "Continuing on the Australian credit dataset, first try pruning the tree with the complexity parameter $\\alpha = 0.1$.  \n",
    "Then try bagging with 10 trees.  \n",
    "Finally try boosting with 10 trees.  \n",
    "\n",
    "Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67592b8-7a74-4421-96aa-3094291ca5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First print out decision tree accuracy on original data\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"DT accuracy with original data: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "# Pruned decision tree on original data\n",
    "clf = tree.DecisionTreeClassifier(random_state=42,ccp_alpha=0.1)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"Pruned decision tree accuracy: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "# Bagged decision tree on original data\n",
    "clf = BaggingClassifier(tree.DecisionTreeClassifier(random_state=42),\n",
    "                            n_estimators=10,random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"Bagged decision tree accuracy: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "# Boosted decision tree on original data\n",
    "clf = AdaBoostClassifier(tree.DecisionTreeClassifier(random_state=42),\n",
    "                         n_estimators=10,random_state=42)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predict)\n",
    "print(\"Boosted decision tree accuracy: \" + \"{0:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bb61ad-709a-411a-a674-f429518b30b1",
   "metadata": {},
   "source": [
    "### Decision Tree, Boosted DT and Bagged DT on DISTINCT\n",
    "\n",
    "Some problems require large representation when represented as a decision tree.\n",
    "One such problem is DISTINCT defined as  \n",
    "$DISTINCT(x,y) = (x_1 \\neq y_1) ~OR~ (x_2 \\neq y_2) \\ldotsâ€€~OR~ (x_d \\neq y_d )$   \n",
    "where $x$ and $y$ are $d$-dimensional binary vectors.  \n",
    "\n",
    "To represent DISTINCT, a decision tree requires at least $2^d$ leaves. Note also, DISTINCT can be represented by a disjunction of $d$ formulas $(x_1 \\neq y_1), \\ldots, (x_d \\neq y_d )$ which is a small representation.\n",
    "\n",
    "We will try learning DISTINCT using a decision tree, boosted decision trees of depth 2, and bagged decision trees of depth 2. Before running the experiment, predict which methods will do well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71bc90-021a-42cd-b196-ba4e37dcce91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 1000\n",
    "input_size = 20\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Construct training and test sets\n",
    "train_data = np.random.randint(2,size=(train_size,input_size))\n",
    "train_data[:,int(input_size/2):] = train_data[:,:int(input_size/2)] # make equal\n",
    "# Create labels\n",
    "train_label = np.zeros(train_size)\n",
    "for i in range(0,train_size):\n",
    "    distinct = np.random.randint(2) # Equally likely to be distinct\n",
    "    if (distinct == 1):\n",
    "        train_label[i] = 1\n",
    "        rand_pos = np.random.randint(input_size/2) # randomly select an input and make it different\n",
    "        train_data[i][rand_pos] = (train_data[i][rand_pos]+1)%2\n",
    "        \n",
    "test_data = np.random.randint(2,size=(test_size,input_size))\n",
    "test_data[:,int(input_size/2):] = test_data[:,:int(input_size/2)] # make equal\n",
    "test_label = np.zeros(test_size)\n",
    "for i in range(0,test_size):\n",
    "    distinct = np.random.randint(2)\n",
    "    if (distinct == 1):\n",
    "        test_label[i] = 1\n",
    "        rand_pos = np.random.randint(input_size/2)\n",
    "        test_data[i][rand_pos] = (test_data[i][rand_pos]+1)%2\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(random_state=42)\n",
    "clf = clf.fit(train_data, train_label)\n",
    "predict = clf.predict(test_data)\n",
    "accuracy = accuracy_score(test_label, predict)\n",
    "print(\"DT accuracy on DISTINCT: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "clf = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2),\n",
    "                         n_estimators=20,random_state=42)\n",
    "clf = clf.fit(train_data, train_label)\n",
    "predict = clf.predict(test_data)\n",
    "accuracy = accuracy_score(test_label, predict)\n",
    "print(\"Boosted decision tree accuracy on DISTINCT: \" + \"{0:.2f}\".format(accuracy))\n",
    "\n",
    "# Bagged decision tree on original data\n",
    "clf = BaggingClassifier(tree.DecisionTreeClassifier(max_depth=2),\n",
    "                            n_estimators=20,random_state=42)\n",
    "clf = clf.fit(train_data, train_label)\n",
    "predict = clf.predict(test_data)\n",
    "accuracy = accuracy_score(test_label, predict)\n",
    "print(\"Bagged decision tree accuracy on DISTINCT: \" + \"{0:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c8300-2e6a-4185-86c6-9a33697958cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b87b51-08da-4cbc-a94f-bc57407afe01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
